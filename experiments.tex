% !TEX root = 15cvpr.tex
\section{Experiments}
In particular, we extract a 4296 dimensional feature vector for each image by concatenating appearance feature and topic distribution. The 4296 dimensional feature vector includes: the second to last layer of Convolutional Neural Network\cite{simonyan2014very} pre-trained on ImageNet \cite{deng2009imagenet}, as the appearance feature, and topic distribution which learned from pLSA \cite{hofmann1999probabilistic}, as topic distribution.

As an illustration, Figure shows the inter-label correlation matrix illustrating the inter-dependency between 33 categories on the SiftFlow dataset. The brighter the block is, the stronger co-occurrence between labels exists. The dark blocks indicate the concepts pairs without correlation on the dataset. 
Among these pairs of the different concepts, we find that the concept pair of television and sofa have strong correlation.

To verify both robustness and effctiveness o our method to noisy annotation condition, we try to add some noise to the initial image-level labels for SiftFlow dataset.


\subsection{Comparison with State of the Art}
In this section, we compare our approach with the existing state of art weakly supervised semantic segmentation methods as well as fully supervised semantic segmentation algorithms on two real-world datasets: PASCAL VOC 2007 \cite{pascal-voc-2007} and SIFT-flow \cite{liu2011nonparametric}.


\begin{table*}[!htp] \small
\label{ExpVOC_test}
\begin{center}
\begin{tabular}{l|l|c| p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm} p{1mm}}

& Methods & \rotatebox{90}{average} & \rotatebox{90}{background} & \rotatebox{90}{aeroplane} & \rotatebox{90}{bicycle} & \rotatebox{90}{bird} & \rotatebox{90}{boat} & \rotatebox{90}{bottle} & \rotatebox{90}{bus} & \rotatebox{90}{car} & \rotatebox{90}{cat} & \rotatebox{90}{chair} & \rotatebox{90}{cow} & \rotatebox{90}{diningtable} & \rotatebox{90}{dog} & \rotatebox{90}{horse} & \rotatebox{90}{motorbike} & \rotatebox{90}{person} & \rotatebox{90}{pottedplant} & \rotatebox{90}{sheep} & \rotatebox{90}{sofa} & \rotatebox{90}{train} & \rotatebox{90}{tv/monitor} \\
\hline
 & Brookes & 9 & \bf{78} & 6 & 0 & 0 & 0 & 0 & 9 & 5 & 10 & 1 & 2 & 11 & 0 & 6 & 6 & 29 & 2 & 2 & 0 & 11 & 0 \\
 Fully & INRIA & 24 & 3 & 1 & 45 & \bf{34} & 16 & \bf{20} & 0 & 68 & \bf{58} & 11 & 0 & \bf{44} & 8 & 1 & 2 & 59 & 37 & 0 & 6 & 19 & 63 \\
 Supervised& MPI & 28 & 3 & \bf{30} & 31 & 10  & \bf{41} & 7 & 8 & 73 & 56 & \bf{37} & \bf{11} & 19 & 2 & 15 & 24 & \bf{67} & 26 & 9 & 3 & 5 & 55\\
 & TKK & \bf{30} & 23 &19 & 21 & 5 & 16 & 3 & 1 & \bf{78} & 1 & 3 & 1 & 23 & \bf{69} & \bf{44} & \bf{42} & 0 & \bf{65} & \bf{30} & \bf{35} & \bf{89} & \bf{71} \\
 & UoCTTI & 21 & 3 & 24 & \bf{53} & 0 & 2 & 16 & \bf{49} & 33 & 1 & 6 & 10 & 0 & 0 & 3 & 21 & 60 & 11 & 0 & 26 & 72 & 58 \\
\hline
Weakly & Zhang \etal \cite{zhang2013sparse} & 24 & $-$ & 48 & 20 & 26 & 25 & 3 & 7 & 23 & 13 & 38 & 19 & 15 & 39 & 17 & 18 & 25 & 47 & 9 & 41 & 17 & 33 \\
Supervised& Ours & 27 & 66 & 26 & 15 & 61 & 12 & 15 & 51 & 30 & 38 & 6 & 29 & 19 & 25 & 29 & 26 & 19 & 12 & 18 & 4 & 28 & 28 \\
\end{tabular}
\caption{Quantitative analysis of VOC2007 results \cite{pascal-voc-2007}, intersection vs. union measure, define as $\frac{TP}{TP + FN + FP}$, in comparison with state-of-the-art methods. The results of fully supervised methods are taken from \cite{pascal-voc-2007}. } 
\label{tab:ExpVOC_test}
\end{center}
\vskip -0.1in
\end{table*}

\textbf{PASCAL VOC 2007}
This dataset was used for the PASCAL Visual Object Category segmentation contest 2007. It is especially challenging for the presence of background clutter, illumination effect and occlusions. It contains 5011 training images, and 4952 test images. Within the training set, for a subset of 422 images which are suitable for evaluation of the segmentation task, the object in these images are marked at pixel level, while the objects in the other images only have the bounding boxes indicating the location of the object and rough boundaries. And there are 20 foreground and 1 background classes in this dataset used for the task of classification, detection, and segmentation.



\textbf{SIFT-flow} The SIFT Flow dataset\cite{liu2011nonparametric} is derived from the LabelMe subset and contains 33 unique semantic labels. It has 2688 images, 2488 used for training and 200 for testing. This dataset mainly consists of outdoor images, which share high similarities in a certain category. So we can easily find the discriminative cues from the images. Average per-class accuracies of our approach and the other related work are reported in Table \ref{tab:ExpSIFTflow_Test}. Due to the well generalization of our approach, we outperform previous weakly-supervised methods and some fully-supervised methods. Farabet won a best performance with deep learning techniques in \cite{farabet2013learning}, which leads the state-of-the-art performance of fully-supervised methods.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Methods & Supervision & Per-Class (\%) \\
\hline
Liu \etal \cite{liu2011nonparametric} & full & 24 \\
Tighe \etal \cite{tighe2010superparsing} & full & 29.4 \\
Tighe \etal \cite{Tighe2013Finding} & full & 39.2 \\
\hline
Vezhnevets \etal \cite{vezhnevets2011weakly} & weak & 14 \\
Vezhnevets \etal \cite{vezhnevets2012weakly} & weak & 21 \\
Zhang \etal \cite{zhang2013sparse} & weak & 26 \\
Xu \etal \cite{xu2014tell} & weak & 27.9 \\
Ours & weak & 30.2 \\
\hline
\end{tabular}
\end{center}
%\vspace{-3mm}
\caption{Quantitative results on the SIFT-flow dataset \cite{liu2011nonparametric}, average per-class recall measure, defined as $\frac{TP}{TP+FN}$, in comparison with state-of-the-art methods. }
\label{tab:ExpSIFTflow_Test}
\end{table}


